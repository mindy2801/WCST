cbind(names(train$Class),table(train$Class),prop.table(table(train$Class))*100)
names(Class)
data.frame(name=names(train$Class),table(train$Class),prop.table(table(train$Class))*100)
table(train$Class)
data.frame(name=names(train$Class),number=table(train$Class),percent=prop.table(table(train$Class))*100)
a <- table(train$Class)
b <- prop.table(table(train$Class))*100
data.frame(name=names(train$Class),number=as.numeric(a),percent=as.numeric(b))
data.frame(name=names(a),number=as.numeric(a),percent=as.numeric(b))
data.frame(name=names(a),freq=a,percent=b)
data.frame(freq=a,percent=b)
data.frame(freq,percent)
freq <- table(train$Class)
percent <- prop.table(table(train$Class))*100
data.frame(freq,percent)
cbind(freq,percent)
data.frame(names(freq),freq,percent)
data.frame(names(freq),count(freq),count(percent))
freq <- table(train$Class)
percent <- prop.table(table(train$Class))*100
cbind(freq,percent)
#univariate plots
boxplot(train)
boxplot(train$SepalLength)
boxplot(train$SepalLength,train$SepalWidth)
boxplot(train$PetalLength)
boxplot(train$SepalLength,main="SepalLength")
#univariate plots
par(mfrow=c(1,4))
boxplot(train$SepalLength,main="SepalLength")
boxplot(train$SepalWidth,main="SepalWidth")
boxplot(train$PetalLength,main="PetalLength")
boxplot(train$PetalWidth,main="PetalLength")
par(mfrow=c(1,4))
boxplot(train$SepalLength,main="Sepal.Length")
boxplot(train$SepalWidth,main="Sepal.Width")
boxplot(train$PetalLength,main="Petal.Length")
boxplot(train$PetalWidth,main="Petal.Length")
boxplot(train, main=names(train))
for (i in c(1:4)) {
boxplot(train[i], main=names(train[i]))
}
par(mfrow=c(1,1))
for (i in c(1:4)) {
boxplot(train[i], main=names(train[i]))
}
plot(train$Class)
#draw class distribution
library(ggplot2)
ggplot(train,aes(Class))+geom_bar(stat="Identity")
ggplot(train,aes(Class))+geom_bar()
for (i in c(1:4)) {
boxplot(train[i], main=names(train[i]))
}
boxplot(train$PetalWidth, main="Petal.Width")
#univariate plots
#draw barplot
par(mfrow=c(1,4))
for (i in c(1:4)) {
boxplot(train[,i], main=names(train[,i]))
}
for (i in c(1:4)) {
boxplot(train[,i], main=names(train[i]))
}
#draw class distribution
par(mfrow=c(1,1))
#scatter plot matrix
x <- train[,1:4]
y <- train[,5]
featurePlot(x=x,y=y,plot="ellipse")
featurePlot(x,y,plot="ellipse")
featurePlot(x,y,plot="pairs")
featurePlot(x,y,plot="scatter")
featurePlot(x,y,plot="density")
density
featurePlot(x,y,plot="strip")
featurePlot(x,y,plot="box")
featurePlot(x,y,plot="ellipse")
featurePlot(x,y,plot="pairs")
featurePlot(x,y,plot="ellipse")
#univariate plots
#draw barplot
par(mfrow=c(1,4))
for (i in c(1:4)) {
boxplot(train[i], main=names(train[i]))
}
#density plot
scales <- list(x=list(relation="free"),y=list(relation="free"))
featurePlot(x,y,plot="density",scales=scales)
#box plot matrix
featurePlot(x,y,plot="box")
relation="free"
featurePlot(x,y,plot="density",scales=scales)
#linear discriminant analysis
ldafit <- train(Class ~., data=train, method="lda", trControl=fitControl)
ldafit
set.seed(100)
cartfit <- train(Class ~ ., data=train, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
cartfit
tree_model <- rpart(Class ~., data=iris, control=rpart.control(cp=0.45))
prp(tree_model)
set.seed(100)
cartfit <- train(Class ~ ., data=iris, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
tree_model <- rpart(Class ~., data=iris, control=rpart.control(cp=0.01))
prp(tree_model)
#k-nearest neighbors
knnfit <- train(Class ~., data=train, method="knn",trControl=fitControl)
knnfit
#support vector machines
svmfit <- train(Class ~., data=train, method="svmRadial", trControl=fitControl)
svmfit
#random forest
set.seed(100)
rffit <- train(Class~., data=train, method="parRF", trControl=fitControl, allowParallel=TRUE)
rf_model <- randomForest(Class~., data=train, mtry=4, ntree=1000)
rffit
rf_model <- randomForest(Class~., data=train, mtry=2, ntree=1000)
#comparison
result <- resamples(list(lda=ldafit, cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
dotplot(result)
bwplot(result)
#linear discriminant analysis
ldafit <- train(Class ~., data=train, method="lda", trControl=fitControl)
ldafit
predict(ldafit, newdata=test)
#k-nearest neighbors
knnfit <- train(Class ~., data=train, method="knn",trControl=fitControl)
knnfit
#support vector machines
svmfit <- train(Class ~., data=train, method="svmRadial", trControl=fitControl)
svmfit
e
#comparison
result <- resamples(list(lda=ldafit, cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
#linear discriminant analysis
set.seed(100)
ldafit <- train(Class ~., data=train, method="lda", trControl=fitControl)
ldafit
#decision tree(CART)
set.seed(100)
cartfit <- train(Class ~ ., data=train, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
cartfit
#random forest
set.seed(100)
rffit <- train(Class~., data=train, method="parRF", trControl=fitControl, allowParallel=TRUE)
rffit
rf_model <- randomForest(Class~., data=train, mtry=2, ntree=1000)
rf_model
tree_model <- rpart(Class ~., data=train, control=rpart.control(cp=0.45))
prp(tree_model)
#k-nearest neighbors
set.seed(100)
knnfit <- train(Class ~., data=train, method="knn",trControl=fitControl)
knnfit
#support vector machines
set.seed(100)
svmfit <- train(Class ~., data=train, method="svmRadial", trControl=fitControl)
svmfit
#comparison
result <- resamples(list(lda=ldafit, cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
dotplot(result)
bwplot(result)
dotplot(result)
#lda results
knnfit
predict(knnfit, newdata=test)
#example
library(xlsx)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LasVegasTripAdvisorReviews.xlsx",sheetIndex=1,header=T)
fitControl <- trainControl(method="cv", number=5)
cartGrid <- expand.grid(cp=(1:50)*0.01)
#linear discriminant analysis
set.seed(100)
ldafit <- train(Class ~., data=train, method="lda", trControl=fitControl)
#decision tree(CART)
set.seed(100)
cartfit <- train(Class ~ ., data=train, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
#random forest
set.seed(100)
rffit <- train(Class~., data=train, method="parRF", trControl=fitControl, allowParallel=TRUE)
#k-nearest neighbors
set.seed(100)
knnfit <- train(Class ~., data=train, method="knn",trControl=fitControl)
#support vector machines
set.seed(100)
svmfit <- train(Class ~., data=train, method="svmRadial", trControl=fitControl)
#comparison
result <- resamples(list(lda=ldafit, cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
#linear discriminant analysis
set.seed(100)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
ldafit
#example
library(xlsx)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LasVegasTripAdvisorReviews.xlsx",sheetIndex=1,header=T)
#linear discriminant analysis
set.seed(100)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LasVegasTripAdvisorReviews.xlsx",sheetIndex=1,header=T)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
fitControl <- trainControl(method="cv", number=5)
cartGrid <- expand.grid(cp=(1:50)*0.01)
#linear discriminant analysis
set.seed(100)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
cartfit <- train(Score ~ ., data=lvtar, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LVTAR.xlsx",sheetIndex=1,header=T)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
fitControl <- trainControl(method="cv", number=5)
cartGrid <- expand.grid(cp=(1:50)*0.01)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
#decision tree
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
#random forest
library(caret)
library(randomForest)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
colnames(lvtar)
#decision tree
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
library(randomForest)
#example
library(xlsx)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LVTAR.xlsx",sheetIndex=1,header=T)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
fitControl <- trainControl(method="cv", number=5)
cartGrid <- expand.grid(cp=(1:50)*0.01)
ldafit <- train(Score ~., data=lvtar, method="lda", trControl=fitControl)
cartfit <- train(Score ~ ., data=lvtar, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
cartfit
rffit <- train(Score~., data=lvtar, method="parRF", trControl=fitControl, allowParallel=TRUE)
rffit
knnfit <- train(Score ~., data=lvtar, method="knn",trControl=fitControl)
knnfit
svmfit <- train(Score ~., data=lvtar, method="svmRadial", trControl=fitControl)
svmfit
ldafit <- train(Score ~ ., data=lvtar, method="lda", trControl=fitControl)
colnames(lvtar)
ldafit <- train(Score ~ Nrreviews+Nrhotelreviews+Helpfulvotes+Hotelstars+Memberyears, data=lvtar, method="lda", trControl=fitControl)
#comparison
result <- resamples(list(cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
dotplot(result)
ldafit <- train(Score ~ Nrreviews+Hotelstars, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Hotelstars ~ Nrreviews+Score+Pool+Gym+Tenniscourt+Spa+Casino, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Hotelstars ~ Score+Pool+Gym+Tenniscourt+Spa+Casino, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Hotelstars ~ Pool+Gym+Tenniscourt+Spa+Casino, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Hotelstars ~ Usercountry+Pool+Gym+Tenniscourt+Spa+Casino+Freeinternet, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Score ~ Usercountry+Pool+Gym+Tenniscourt+Spa+Casino+Freeinternet, data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Score ~ ., data=lvtar, method="lda")
fitControl <- trainControl(method="cv", number=5)
ldafit <- train(Score ~ ., data=lvtar, method="lda")
ldafit <- train(Score ~ ., data=lvtar, method="lda", trControl=fitControl)
ldafit <- train(Score ~ Hotelstars, data=lvtar, method="lda", trControl=fitControl)
lvtar1 <- read.xlsx("C:/Users/desk1/Desktop/정효림/LVTAR.xlsx",sheetIndex=1,header=T)
nrow(lvtar1)
i <- createDataPartition(lvtar1$Score, p=0.80, list=FALSE)
lvtar <- iris[i,]
test <- iris[-i,]
fitControl <- trainControl(method="cv", number=5)
cartGrid <- expand.grid(cp=(1:50)*0.01)
ldafit <- train(Score ~ ., data=lvtar, method="lda", trControl=fitControl)
colnames(lvtar)
lvtar <- lvtar1[i,]
test <- lvtar1[-i,]
ldafit <- train(Score ~ ., data=lvtar, method="lda", trControl=fitControl)
cartfit <- train(Score ~ ., data=lvtar, method="rpart", trControl=fitControl, tuneGrid=cartGrid)
cartfit
lvtar
rffit <- train(Score~ ., data=lvtar, method="parRF", trControl=fitControl, allowParallel=TRUE)
rffit
knnfit <- train(Score ~., data=lvtar, method="knn",trControl=fitControl)
knnfit
svmfit <- train(Score ~., data=lvtar, method="svmRadial", trControl=fitControl)
svmfit
#comparison
result <- resamples(list(cart=cartfit, knn=knnfit, svm=svmfit, rf=rffit))
summary(result)
dotplot(result)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LVTAR.xlsx",sheetIndex=1,header=T)
lvtar <- read.xlsx("C:/Users/desk1/Desktop/정효림/LVTAR.xlsx",sheetIndex=1,header=T)
x<-2
y<-5
x+y
source('~/.active-rstudio-document', echo=TRUE)
library(bigmemory)
library(biganalytics)
library(bigtabulate)
path = "C:\Users\desk1\Desktop\정효림\"
read.csv("C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv")
filename <- read.csv("C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv")
bigfile.sample <- read.csv(filename, stringsAsFactors = F, header=T, nrows=20)
path = "C:/Users/desk1/Desktop/정효림/"
filename <- sprintf("%snumeric_SAT_Results2014.csv",path)
bigfile.sample <- read.csv(filename, stringsAsFactors = F, header=T, nrows=20)
bigfile.raw <- read.csv(filename, stringsAsFactors = F, header=T, nrow=10000)
school.matrix <- read.big.matrix(filename, type="integer", header=T, backingfile ="school.bin", descriptorfile = "school.desc",extraCols=NULL)
desc <- describe(school.matrix)
str(school.matrix)
sum(as.numeric(school.matrix[,3]),na.rm=T)
dput(desc, file="A.desc")
shared.desc <- dget("A.desc")
shared.bigobject <- attach.big.matrix(shared.desc)
sum(shared.bigobject[,3], na.rm=T)
#ffdf
library(ff)
school.ff <- read.csv.ffdf(filename, header=T)
school.ff <- read.csv.ffdf(filename, header=F)
school.ff <- read.csv.ffdf(filename, first.rows=F)
school.ff <- read.csv.ffdf(file="C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv", header=F)
class(school.ff)
str(school.ff)
sum(school.ff[,1],na.rm=T)
View(school.ff)
school.ff <- read.csv.ffdf(file="C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv", header=T)
class(school.ff)
str(school.ff)
sum(school.ff[,1],na.rm=T)
View(school.ff)
sum(as.numeric(school.ff[,1]),na.rm=T)
#ffdf
library(ff)
school.ff <- read.csv.ffdf(file="C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv", header=T)
class(school.ff)
str(school.ff)
sum(as.numeric(school.ff[,1]),na.rm=T)
school.ff <- read.csv.ffdf(file="C:/Users/desk1/Desktop/정효림/numeric_SAT_Results2014.csv", header=F)
class(school.ff)
str(school.ff)
sum(as.numeric(school.ff[,1]),na.rm=T)
c(1,2,4)^2
a <- matrix(1, nrow=2, ncol=2)
a
b <- matrix(c(1,2,3,4), nrow=2, ncol=2)
b
a*b
a<-b
a*b
a<-array(0,c(3,4,2))
a
a[1,,1]
6+6
A+B
6+6
A<-9
B<-12
A+B
getwd()
wd <- "C:/Users/desk1/Desktop"
wd <- "C:/Users/desk1/Desktop/WCST-master"
setwd(wd)
rm(list=ls())
list()
dir()
#calculate categories completed
subj_catecom <- c()
datname="choices correct Bechara SDIandcontrols.txt"
maxiter=100 				#Number of starting parameters to iterate through; default is 100
modelstorun=5    #lists the nested models that'll be run; #5 is the default, best fitting model in article; use "modelstorun=1:24" to run all (slower)
parbounds=c(0,0,.01,.01,1,1,5,5)  #lower boundaries for parameters r, p, d, i and upper boundaries for parameters r, p, d, i
#############################
if(sum(rownames(installed.packages())=="fOptions")==0)   #If the fOptions package is not installed, .
install.packages("fOptions") 				#.install it now.
library(fOptions)  						#load fOptions into memory.
rawdatamat=read.table(datname,encoding="UTF-8")
#If you receive an error after this line, then the datafile isn't being read in properly.
#One possible problem is that the datafile doesn't have UTF-8 Encoding.
#One solution is to use the Notepad program to resave the datafile.
#At the bottom of the Save dialog window, next to "Encoding:", choose "UTF-8" before saving the datafile.
#If that does not fix the problem, try opening the file in MS Word (using windows default encoding),
#delete the unusual characters, and then resave it (again using windows default encoding).
rawdatamat[,257] <- 1:88
rawdatamat[,258] <- c(rep("SDI",39),rep("Control", 88-39))
subjlabels=rawdatamat[,257]				#reads extra information in datafile if available
subjgroup= rawdatamat[,258]
subjectsmodeled=1:length(rawdatamat[,1])
subjectsmodeled
for (subj in subjectsmodeled){
inf <- data.frame(rawdatamat[subj,129:(128+lengthvec[subj])])
catecom=0
locvec <- which(inf==0)
for(i in 1:(length(locvec)-1)) {
if(sum(inf[locvec[i]:locvec[i+1]])>=10){catecom=catecom+1}
if((i == (length(locvec)-1)) & sum(inf[locvec[i+1]:length(inf)])==10){catecom=catecom+1}
}
subj_catecom[subj] <- catecom
}
lengthvec=128-rowSums(rawdatamat[,1:128]==0) #number of trials
lengthvec
for (subj in subjectsmodeled){
inf <- data.frame(rawdatamat[subj,129:(128+lengthvec[subj])])
catecom=0
locvec <- which(inf==0)
for(i in 1:(length(locvec)-1)) {
if(sum(inf[locvec[i]:locvec[i+1]])>=10){catecom=catecom+1}
if((i == (length(locvec)-1)) & sum(inf[locvec[i+1]:length(inf)])==10){catecom=catecom+1}
}
subj_catecom[subj] <- catecom
}
subj_catecom
##data frame
control_catecom <- subj_catecom[1:39]
sdi_catecom <- mean(subj_catecom[40:88])
data.frame(group=c("Control", "SDI"), score=rep("Categoreis Completed",2), value=c(control_catecom, sdi_catecom))
subj_catecom_dat <- data.frame(group=c("Control", "SDI"), score=rep("Categoreis Completed",2), value=c(control_catecom, sdi_catecom))
subj_catecom_dat
control_catecom
##data frame
sdi_catecom <- subj_catecom[1:39]
control_catecom <- mean(subj_catecom[40:88])
subj_catecom_dat <- data.frame(group=c(rep("SDI", 39), rep("Control",88-39)), score=rep("Categoreis Completed",88), value=c(control_catecom, sdi_catecom))
88-39
subj_catecom_dat <- data.frame(group=c(rep("SDI", 39), rep("Control",88-39)), score=rep("Categoreis Completed",88), value=subjcatecom))
subj_catecom_dat <- data.frame(group=c(rep("SDI", 39), rep("Control",88-39)), score=rep("Categoreis Completed",88), value=subj_catecom))
subj_catecom_dat <- data.frame(group=c(rep("SDI", 39), rep("Control",88-39)), score=rep("Categoreis Completed",88), value=subj_catecom)
subj_catecom_dat
#failure to maintain set
subj_failure <- c()
for (subj in subjectsmodeled){
inf <- data.frame(rawdatamat[subj,129:(128+lengthvec[subj])])
failure=0
locvec <- which(inf==0)
for(i in 1:(length(locvec)-1)) {
if((sum(inf[locvec[i]:locvec[i+1]])<10 & sum(inf[locvec[i]:locvec[i+1]]) >= 5)){failure = failure+1}
}
subj_failure[subj] <- failure
}
subj_failure
##data frame
sdi_catecom <- mean(subj_catecom[1:39])
control_catecom <- mean(subj_catecom[40:88])
subj_catecom_dat <- data.frame(group=c("SDI", "Control"), score=rep("Categoreis Completed",2), value=subj_catecom)
subj_catecom_dat
mean_catecom <- c(sdi_catecom, control_catecom)
subj_catecom_dat <- data.frame(group=c("SDI", "Control"), score=rep("Categoreis Completed",2), value=mean_catecom)
subj_catecom_dat
##data frame
sdi_failure <- mean(subj_failure[1:39])
control_failure <- mean(subj_failure[40:88])
mean_failure <- c(sdi_failure, control_failure)
subj_failure_dat <- data.frame(group=c("SDI", "Control"), score=rep("Set Failures",2), value=mean_failure)
subj_catecom
subj_failure_dat
#integrate
catecom_sd <- c(sd(subj_catecom[1:39]),sd(subj_catecom[40:88]))
catecom_sd
failure_sd <- c(sd(subj_failure[1:39]),sd(subj_failure[40:88]))
total <- rbind(subj_catecom, subj_failure);
total
total <- rbind(subj_catecom_dat, subj_failure_dat)
total
rbind(catecom_sd, failure_sd)
c(catecom_sd, failure_sd)
sd <- c(catecom_sd, failure_sd)
cbind(total, sd)
total2 <- cbind(total, sd)
total2
total <- rbind(subj_catecom_dat, subj_failure_dat); total <- cbind(total, sd)
total
library(ggplot2)
bp <- ggplot(total, aes(x=score, y=value, fill=group)) +
geom_bar(colour="black", stat="identity", position=position_dodge(), width=0.5) +
geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=0.1,
position=position_dodge(.5))
bp + labs(title="(A) Observed Data", x="", y="Observed Value") +
scale_fill_manual(values=c('black','white'), guide = guide_legend(title=NULL)) +
theme_classic() +
theme(legend.position = c(0.75,0.75))
bp
library(ggplot2)
bp <- ggplot(total, aes(x=score, y=value, fill=group)) +
geom_bar(colour="black", stat="identity", position=position_dodge(), width=0.5) +
geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=0.1,
position=position_dodge(.5))
bp + labs(title="(A) Observed Data", x="", y="Observed Value") +
scale_fill_manual(values=c('black','white'), guide = guide_legend(title=NULL)) +
theme_classic() +
theme(legend.position = c(0.75,0.75))
#scale_y_continuous(expand = c(0, 0), limits = c(0, 6.5)) for deleting the space of x-axis. It cuases problem now because of 0 value of "Set Failures"
bp
bp <- ggplot(total, aes(x=score, y=value, fill=group)) +
geom_bar(colour="black", stat="identity", position=position_dodge(), width=0.5) +
geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=0.1,
position=position_dodge(.5))
bp + labs(title="(A) Observed Data", x="", y="Observed Value") +
scale_fill_manual(values=c('black','white'), guide = guide_legend(title=NULL)) +
theme_classic() +
theme(legend.position = c(0.75,0.75)) +
scale_y_continuous(expand = c(0, 0), limits = c(0, 6.5)) #for deleting the space of x-axis. It cuases problem now because of 0 value of "Set Failures"
bp
bp
bp <- ggplot(total, aes(x=score, y=value, fill=group)) +
geom_bar(colour="black", stat="identity", position=position_dodge(), width=0.5) +
geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=0.1,
position=position_dodge(.5))
bp + labs(title="(A) Observed Data", x="", y="Observed Value") +
scale_fill_manual(values=c('black','white'), guide = guide_legend(title=NULL)) +
theme_classic() +
theme(legend.position = c(0.75,0.75)) +
scale_y_continuous(expand = c(0, 0), limits = c(0, 6.5)) #for deleting the space of x-axis. It cuases problem now because of 0 value of "Set Failures"
bp
bp
